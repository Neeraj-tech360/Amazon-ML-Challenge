{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13365853,"sourceType":"datasetVersion","datasetId":8478655}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y pillow\n!pip install pillow==10.3.0\n!pip install -U sentence-transformers transformers lightgbm xgboost tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:02:29.695435Z","iopub.execute_input":"2025-10-13T10:02:29.696094Z","iopub.status.idle":"2025-10-13T10:05:45.063053Z","shell.execute_reply.started":"2025-10-13T10:02:29.696074Z","shell.execute_reply":"2025-10-13T10:05:45.062220Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pillow 11.3.0\nUninstalling pillow-11.3.0:\n  Successfully uninstalled pillow-11.3.0\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78e69d2bd450>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pillow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78e69d242710>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pillow/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78e69d2383d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/pillow/\u001b[0m\u001b[33m\n\u001b[0mCollecting pillow==10.3.0\n  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pillow\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pillow-10.3.0\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nCollecting xgboost\n  Downloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.0.0rc2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, xgboost, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.1 tokenizers-0.22.1 transformers-4.57.0 xgboost-3.0.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Configuration for Kaggle\nimport os\nKAGGLE_INPUT_DIR = '/kaggle/input/infiniper'   # change if folder name differs\nKAGGLE_WORKING_DIR = '/kaggle/working/'\n\nDATA_DIR = os.path.join(KAGGLE_INPUT_DIR, 'dataset')\nTEXT_EMB_DIR = os.path.join(KAGGLE_INPUT_DIR, 'embeddings_text')\nIMAGE_EMB_DIR = os.path.join(KAGGLE_INPUT_DIR, 'embeddings_image')\n\nprint(\"Paths:\")\nprint(\" DATA_DIR:\", DATA_DIR)\nprint(\" TEXT_EMB_DIR:\", TEXT_EMB_DIR)\nprint(\" IMAGE_EMB_DIR:\", IMAGE_EMB_DIR)\nprint(\" WORKING_DIR:\", KAGGLE_WORKING_DIR)\n\n# Imports\nimport os, sys, re, gc, math, time\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# SMAPE\ndef smape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    denom = np.where(denom == 0, eps, denom)\n    return np.mean(np.abs(y_pred - y_true) / denom) * 100.0\n\n# Utility: load numpy with fallback\ndef load_npy_or_fail(path):\n    if os.path.exists(path):\n        return np.load(path)\n    else:\n        raise FileNotFoundError(f\"Required file missing: {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:06:09.243574Z","iopub.execute_input":"2025-10-13T10:06:09.244387Z","iopub.status.idle":"2025-10-13T10:06:11.221038Z","shell.execute_reply.started":"2025-10-13T10:06:09.244342Z","shell.execute_reply":"2025-10-13T10:06:11.220441Z"}},"outputs":[{"name":"stdout","text":"Paths:\n DATA_DIR: /kaggle/input/infiniper/dataset\n TEXT_EMB_DIR: /kaggle/input/infiniper/embeddings_text\n IMAGE_EMB_DIR: /kaggle/input/infiniper/embeddings_image\n WORKING_DIR: /kaggle/working/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load CSVs\nprint(\"Loading CSVs...\")\ntrain = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ntest  = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\nsample_test = pd.read_csv(os.path.join(DATA_DIR, 'sample_test.csv'))\nprint(\"Train/Test/Sample shapes:\", train.shape, test.shape, sample_test.shape)\n\n# Load embeddings (npy files). Adjust file names if different\nprint(\"Loading embeddings...\")\ntrain_text_emb = load_npy_or_fail(os.path.join(TEXT_EMB_DIR, 'train_text_emb.npy'))\ntest_text_emb  = load_npy_or_fail(os.path.join(TEXT_EMB_DIR, 'test_text_emb.npy'))\nsample_text_emb= load_npy_or_fail(os.path.join(TEXT_EMB_DIR, 'sample_text_emb.npy'))\n\ntrain_img_emb = load_npy_or_fail(os.path.join(IMAGE_EMB_DIR, 'train_img_emb.npy'))\ntest_img_emb  = load_npy_or_fail(os.path.join(IMAGE_EMB_DIR, 'test_img_emb.npy'))\nsample_img_emb= load_npy_or_fail(os.path.join(IMAGE_EMB_DIR, 'sample_img_emb.npy'))\n\nprint(\"Text emb shapes:\", train_text_emb.shape, test_text_emb.shape, sample_text_emb.shape)\nprint(\"Image emb shapes:\", train_img_emb.shape, test_img_emb.shape, sample_img_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:06:16.225327Z","iopub.execute_input":"2025-10-13T10:06:16.225718Z","iopub.status.idle":"2025-10-13T10:06:22.597968Z","shell.execute_reply.started":"2025-10-13T10:06:16.225697Z","shell.execute_reply":"2025-10-13T10:06:22.597125Z"}},"outputs":[{"name":"stdout","text":"Loading CSVs...\nTrain/Test/Sample shapes: (75000, 4) (75000, 3) (100, 3)\nLoading embeddings...\nText emb shapes: (75000, 384) (75000, 384) (100, 384)\nImage emb shapes: (75000, 512) (75000, 512) (100, 512)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Ensure catalog_content present and compute simple features\nfor df, name in [(train,'train'), (test,'test'), (sample_test,'sample_test')]:\n    if 'catalog_content' not in df.columns:\n        df['catalog_content'] = ''\n    df['catalog_content'] = df['catalog_content'].fillna('')\n    df['char_len'] = df['catalog_content'].str.len()\n    df['word_count'] = df['catalog_content'].str.split().apply(len)\n    df['num_digits'] = df['catalog_content'].str.count(r'\\d').fillna(0).astype(int)\n\n# parse Value and Unit (simple regex)\ndef parse_value_unit(text):\n    val = np.nan; unit = ''\n    if not isinstance(text, str): return val, unit\n    m = re.search(r'Value[:\\s]*([0-9]+(?:\\.[0-9]+)?)', text, flags=re.I)\n    if m:\n        try: val = float(m.group(1))\n        except: val = np.nan\n    m2 = re.search(r'Unit[:\\s]*([A-Za-z0-9% /._-]+)', text, flags=re.I)\n    if m2:\n        unit = m2.group(1).strip()\n    return val, unit\n\n# Run parsing with tqdm\nfor df, name in [(train,'train'), (test,'test'), (sample_test,'sample_test')]:\n    vals = []\n    units = []\n    for txt in tqdm(df['catalog_content'].tolist(), desc=f'parse {name}', ncols=100):\n        v,u = parse_value_unit(txt)\n        vals.append(v); units.append(u)\n    df['value_extracted'] = vals\n    df['unit_extracted'] = units\n\nprint(\"Sample feature columns:\")\ndisplay(train[['sample_id','value_extracted','unit_extracted','char_len','word_count']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:06:26.446108Z","iopub.execute_input":"2025-10-13T10:06:26.446456Z","iopub.status.idle":"2025-10-13T10:06:34.951306Z","shell.execute_reply.started":"2025-10-13T10:06:26.446434Z","shell.execute_reply":"2025-10-13T10:06:34.950637Z"}},"outputs":[{"name":"stderr","text":"parse train: 100%|█████████████████████████████████████████| 75000/75000 [00:01<00:00, 49684.01it/s]\nparse test: 100%|██████████████████████████████████████████| 75000/75000 [00:01<00:00, 51395.54it/s]\nparse sample_test: 100%|███████████████████████████████████████| 100/100 [00:00<00:00, 41901.14it/s]","output_type":"stream"},{"name":"stdout","text":"Sample feature columns:\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   sample_id  value_extracted unit_extracted  char_len  word_count\n0      33127            72.00          Fl Oz        91          18\n1     198967            32.00          Ounce       511          80\n2     261251            11.40          Ounce       328          59\n3      55858            11.25          Ounce      1318         211\n4     292686            12.00          Count       155          28","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>value_extracted</th>\n      <th>unit_extracted</th>\n      <th>char_len</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>72.00</td>\n      <td>Fl Oz</td>\n      <td>91</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>32.00</td>\n      <td>Ounce</td>\n      <td>511</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>11.40</td>\n      <td>Ounce</td>\n      <td>328</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>55858</td>\n      <td>11.25</td>\n      <td>Ounce</td>\n      <td>1318</td>\n      <td>211</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>292686</td>\n      <td>12.00</td>\n      <td>Count</td>\n      <td>155</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# numeric features\ndef numeric_array(df):\n    return df[['char_len','word_count','num_digits','value_extracted']].fillna(0).to_numpy(dtype=float)\n\n# Stack features: [text_emb, img_emb, numeric]\ndef build_X(text_emb, img_emb, df):\n    num = numeric_array(df)\n    return np.hstack([text_emb, img_emb, num])\n\n# Build\nX_train_raw = build_X(train_text_emb, train_img_emb, train)\nX_test_raw  = build_X(test_text_emb, test_img_emb, test)\nX_sample_raw= build_X(sample_text_emb, sample_img_emb, sample_test)\ny = train['price'].values\ny_log = np.log1p(y)\n\nprint(\"Raw shapes:\", X_train_raw.shape, X_test_raw.shape, X_sample_raw.shape)\n\n# Scale for MLP only (fit on train)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train_raw)\nX_test  = scaler.transform(X_test_raw)\nX_sample= scaler.transform(X_sample_raw)\n\n# Keep raw for LightGBM since tree-based models don't need scaling\nX_train_lgb = X_train_raw\nX_test_lgb  = X_test_raw\nX_sample_lgb= X_sample_raw\n\nprint(\"Prepared X (MLP scaled) shape:\", X_train.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:06:42.504331Z","iopub.execute_input":"2025-10-13T10:06:42.505081Z","iopub.status.idle":"2025-10-13T10:06:45.646509Z","shell.execute_reply.started":"2025-10-13T10:06:42.505053Z","shell.execute_reply":"2025-10-13T10:06:45.645691Z"}},"outputs":[{"name":"stdout","text":"Raw shapes: (75000, 900) (75000, 900) (100, 900)\nPrepared X (MLP scaled) shape: (75000, 900)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nfrom tqdm import tqdm\n\n# ---------- Robust SMAPE eval (handles both (preds, dataset) and (labels, preds)) ----------\ndef lgb_smape_eval(a, b):\n    \"\"\"\n    Robust wrapper for LightGBM custom evaluation.\n    Accepts either:\n      - (preds, dataset) where dataset has get_label()\n      - (labels, preds) where both are numpy arrays (sklearn wrapper may call like this)\n    Returns ('SMAPE', value, False) which LightGBM accepts.\n    \"\"\"\n    # Case 1: b is a Dataset-like object (has get_label)\n    if hasattr(b, 'get_label'):\n        y_pred_log = np.asarray(a)\n        y_true_log = np.asarray(b.get_label())\n    else:\n        # Case 2: sklearn wrapper often calls func(labels, preds)\n        # Here 'a' are labels and 'b' are preds (both numpy arrays)\n        # But to be safe, detect which arg is preds by shape: preds is often float vector (but same shape)\n        # We'll assume a = labels, b = preds (this matches LightGBM's sklearn wrapper behavior)\n        y_true_log = np.asarray(a)\n        y_pred_log = np.asarray(b)\n\n    # Now both are log1p(target) because we train on log1p(y)\n    # Convert back to original scale before SMAPE\n    y_true = np.expm1(y_true_log)\n    y_pred = np.expm1(y_pred_log)\n\n    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    denom = np.where(denom == 0, 1e-8, denom)\n    val = np.mean(np.abs(y_pred - y_true) / denom)\n    # Return triple (name, value, is_higher_better) -> False because lower SMAPE is better\n    return 'SMAPE', val, False\n\n# ---------- LightGBM params & CV setup ----------\nparams = {\n    'objective':'regression', 'metric':'None',\n    'learning_rate': 0.02, 'n_estimators':5000, 'num_leaves':512,\n    'min_data_in_leaf':30, 'feature_fraction':0.8, 'bagging_fraction':0.8, 'bagging_freq':5,\n    'lambda_l1':0.5, 'lambda_l2':0.5, 'n_jobs':-1, 'verbosity':-1\n}\n\n# Stratify by log price bins\nn_bins = 10\ntry:\n    bins = pd.qcut(np.log1p(train['price']), q=n_bins, labels=False, duplicates='drop')\nexcept Exception:\n    bins = pd.cut(np.log1p(train['price']), bins=n_bins, labels=False)\nbins = np.array(bins, dtype=int)\n\nseeds = [42, 2023]   # bagging / seeds\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Storage\npreds_oof_log = np.zeros(len(train))\npreds_test_log = np.zeros(len(test))\npreds_sample_log = np.zeros(len(sample_test))\n\nfold_idx = 0\nfold_smape_list = []\n\n# Quick shape sanity\nprint(\"Shapes: X_train:\", X_train_lgb.shape, \"X_test:\", X_test_lgb.shape, \"y:\", len(y))\n\nfor train_idx, val_idx in tqdm(skf.split(X_train_lgb, bins), total=n_splits, desc='LGB CV folds', ncols=100):\n    fold_idx += 1\n    print(f\"\\n--- Fold {fold_idx} ---\")\n    X_tr, X_val = X_train_lgb[train_idx], X_train_lgb[val_idx]\n    y_tr_log, y_val_log = np.log1p(y[train_idx]), np.log1p(y[val_idx])\n\n    fold_val_preds_log = np.zeros(len(val_idx))\n    fold_test_preds_log = np.zeros(X_test_lgb.shape[0])\n    fold_sample_preds_log = np.zeros(X_sample_lgb.shape[0])\n\n    for seed in seeds:\n        print(f\" Training seed {seed} ...\")\n        params['random_state'] = seed\n        model = lgb.LGBMRegressor(**params)\n\n        # Fit using sklearn wrapper and our robust custom eval\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_val, y_val_log)],\n            eval_metric=lgb_smape_eval,\n            callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(period=200)]\n        )\n\n        # Predictions (log scale)\n        val_pred_log = model.predict(X_val, num_iteration=model.best_iteration_)\n        test_pred_log = model.predict(X_test_lgb, num_iteration=model.best_iteration_)\n        sample_pred_log = model.predict(X_sample_lgb, num_iteration=model.best_iteration_)\n\n        fold_val_preds_log += val_pred_log\n        fold_test_preds_log += test_pred_log\n        fold_sample_preds_log += sample_pred_log\n\n        del model\n        gc.collect()\n\n    # Average over seeds\n    fold_val_preds_log /= len(seeds)\n    fold_test_preds_log /= len(seeds)\n    fold_sample_preds_log /= len(seeds)\n\n    preds_oof_log[val_idx] = fold_val_preds_log\n    preds_test_log += fold_test_preds_log / n_splits\n    preds_sample_log += fold_sample_preds_log / n_splits\n\n    # Compute fold SMAPE on original scale\n    fold_val_orig = np.expm1(fold_val_preds_log)\n    fold_true_orig = np.expm1(y_val_log)\n    fold_smape = smape(fold_true_orig, fold_val_orig)\n    fold_smape_list.append(fold_smape)\n    print(f\" Fold {fold_idx} SMAPE: {fold_smape:.4f}%\")\n\n# Final inverse transform\npreds_val = np.expm1(preds_oof_log)\npreds_test = np.expm1(preds_test_log)\npreds_sample = np.expm1(preds_sample_log)\n\ncv_smape = smape(y, preds_val)\nprint(f\"\\nCV OOF SMAPE: {cv_smape:.4f}%\")\nprint(\"Per-fold SMAPEs:\", fold_smape_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:09:17.363501Z","iopub.execute_input":"2025-10-13T10:09:17.364089Z","iopub.status.idle":"2025-10-13T16:46:53.144937Z","shell.execute_reply.started":"2025-10-13T10:09:17.364064Z","shell.execute_reply":"2025-10-13T16:46:53.143990Z"}},"outputs":[{"name":"stdout","text":"Shapes: X_train: (75000, 900) X_test: (75000, 900) y: 75000\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds:   0%|                                                           | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Fold 1 ---\n Training seed 42 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.561945\n[400]\tvalid_0's SMAPE: 0.544401\n[600]\tvalid_0's SMAPE: 0.538629\n[800]\tvalid_0's SMAPE: 0.536217\n[1000]\tvalid_0's SMAPE: 0.53496\n[1200]\tvalid_0's SMAPE: 0.534297\n[1400]\tvalid_0's SMAPE: 0.53395\n[1600]\tvalid_0's SMAPE: 0.533769\n[1800]\tvalid_0's SMAPE: 0.533581\n[2000]\tvalid_0's SMAPE: 0.533485\n[2200]\tvalid_0's SMAPE: 0.533422\n[2400]\tvalid_0's SMAPE: 0.533369\n[2600]\tvalid_0's SMAPE: 0.533337\n[2800]\tvalid_0's SMAPE: 0.53332\n[3000]\tvalid_0's SMAPE: 0.533301\n[3200]\tvalid_0's SMAPE: 0.533282\n[3400]\tvalid_0's SMAPE: 0.533268\n[3600]\tvalid_0's SMAPE: 0.533264\n[3800]\tvalid_0's SMAPE: 0.533253\n[4000]\tvalid_0's SMAPE: 0.533243\n[4200]\tvalid_0's SMAPE: 0.533231\n[4400]\tvalid_0's SMAPE: 0.53323\n[4600]\tvalid_0's SMAPE: 0.533226\n[4800]\tvalid_0's SMAPE: 0.533223\n[5000]\tvalid_0's SMAPE: 0.533216\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's SMAPE: 0.533216\n Training seed 2023 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.562661\n[400]\tvalid_0's SMAPE: 0.544964\n[600]\tvalid_0's SMAPE: 0.53916\n[800]\tvalid_0's SMAPE: 0.53666\n[1000]\tvalid_0's SMAPE: 0.535433\n[1200]\tvalid_0's SMAPE: 0.534805\n[1400]\tvalid_0's SMAPE: 0.534527\n[1600]\tvalid_0's SMAPE: 0.534293\n[1800]\tvalid_0's SMAPE: 0.534153\n[2000]\tvalid_0's SMAPE: 0.534076\n[2200]\tvalid_0's SMAPE: 0.533999\n[2400]\tvalid_0's SMAPE: 0.533957\n[2600]\tvalid_0's SMAPE: 0.533915\n[2800]\tvalid_0's SMAPE: 0.533884\n[3000]\tvalid_0's SMAPE: 0.533859\n[3200]\tvalid_0's SMAPE: 0.533844\n[3400]\tvalid_0's SMAPE: 0.533827\n[3600]\tvalid_0's SMAPE: 0.533819\n[3800]\tvalid_0's SMAPE: 0.533811\n[4000]\tvalid_0's SMAPE: 0.533803\n[4200]\tvalid_0's SMAPE: 0.533799\n[4400]\tvalid_0's SMAPE: 0.533795\n[4600]\tvalid_0's SMAPE: 0.533791\n[4800]\tvalid_0's SMAPE: 0.533787\n[5000]\tvalid_0's SMAPE: 0.533784\nDid not meet early stopping. Best iteration is:\n[4933]\tvalid_0's SMAPE: 0.533783\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds:  20%|█████████                                    | 1/5 [1:21:30<5:26:01, 4890.50s/it]","output_type":"stream"},{"name":"stdout","text":" Fold 1 SMAPE: 53.1606%\n\n--- Fold 2 ---\n Training seed 42 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.557245\n[400]\tvalid_0's SMAPE: 0.539846\n[600]\tvalid_0's SMAPE: 0.534096\n[800]\tvalid_0's SMAPE: 0.531489\n[1000]\tvalid_0's SMAPE: 0.530301\n[1200]\tvalid_0's SMAPE: 0.529713\n[1400]\tvalid_0's SMAPE: 0.529344\n[1600]\tvalid_0's SMAPE: 0.529142\n[1800]\tvalid_0's SMAPE: 0.529005\n[2000]\tvalid_0's SMAPE: 0.528902\n[2200]\tvalid_0's SMAPE: 0.528861\n[2400]\tvalid_0's SMAPE: 0.528806\n[2600]\tvalid_0's SMAPE: 0.528782\n[2800]\tvalid_0's SMAPE: 0.528751\n[3000]\tvalid_0's SMAPE: 0.528731\n[3200]\tvalid_0's SMAPE: 0.528713\n[3400]\tvalid_0's SMAPE: 0.528704\n[3600]\tvalid_0's SMAPE: 0.528694\n[3800]\tvalid_0's SMAPE: 0.528689\n[4000]\tvalid_0's SMAPE: 0.528683\n[4200]\tvalid_0's SMAPE: 0.528674\n[4400]\tvalid_0's SMAPE: 0.528665\n[4600]\tvalid_0's SMAPE: 0.528662\n[4800]\tvalid_0's SMAPE: 0.528655\n[5000]\tvalid_0's SMAPE: 0.528652\nDid not meet early stopping. Best iteration is:\n[4974]\tvalid_0's SMAPE: 0.528651\n Training seed 2023 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.555137\n[400]\tvalid_0's SMAPE: 0.53767\n[600]\tvalid_0's SMAPE: 0.532008\n[800]\tvalid_0's SMAPE: 0.529658\n[1000]\tvalid_0's SMAPE: 0.528514\n[1200]\tvalid_0's SMAPE: 0.527912\n[1400]\tvalid_0's SMAPE: 0.52763\n[1600]\tvalid_0's SMAPE: 0.527441\n[1800]\tvalid_0's SMAPE: 0.527321\n[2000]\tvalid_0's SMAPE: 0.527244\n[2200]\tvalid_0's SMAPE: 0.527176\n[2400]\tvalid_0's SMAPE: 0.527133\n[2600]\tvalid_0's SMAPE: 0.527091\n[2800]\tvalid_0's SMAPE: 0.527066\n[3000]\tvalid_0's SMAPE: 0.527047\n[3200]\tvalid_0's SMAPE: 0.527034\n[3400]\tvalid_0's SMAPE: 0.527028\n[3600]\tvalid_0's SMAPE: 0.527016\n[3800]\tvalid_0's SMAPE: 0.527008\n[4000]\tvalid_0's SMAPE: 0.527002\n[4200]\tvalid_0's SMAPE: 0.526997\n[4400]\tvalid_0's SMAPE: 0.526989\n[4600]\tvalid_0's SMAPE: 0.52699\nEarly stopping, best iteration is:\n[4428]\tvalid_0's SMAPE: 0.526988\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds:  40%|██████████████████                           | 2/5 [2:36:44<3:53:27, 4669.31s/it]","output_type":"stream"},{"name":"stdout","text":" Fold 2 SMAPE: 52.5957%\n\n--- Fold 3 ---\n Training seed 42 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.558453\n[400]\tvalid_0's SMAPE: 0.540671\n[600]\tvalid_0's SMAPE: 0.534977\n[800]\tvalid_0's SMAPE: 0.532207\n[1000]\tvalid_0's SMAPE: 0.530897\n[1200]\tvalid_0's SMAPE: 0.53022\n[1400]\tvalid_0's SMAPE: 0.529897\n[1600]\tvalid_0's SMAPE: 0.529714\n[1800]\tvalid_0's SMAPE: 0.529546\n[2000]\tvalid_0's SMAPE: 0.529448\n[2200]\tvalid_0's SMAPE: 0.529385\n[2400]\tvalid_0's SMAPE: 0.529331\n[2600]\tvalid_0's SMAPE: 0.529303\n[2800]\tvalid_0's SMAPE: 0.529282\n[3000]\tvalid_0's SMAPE: 0.529261\n[3200]\tvalid_0's SMAPE: 0.529247\n[3400]\tvalid_0's SMAPE: 0.529238\n[3600]\tvalid_0's SMAPE: 0.529229\n[3800]\tvalid_0's SMAPE: 0.529222\n[4000]\tvalid_0's SMAPE: 0.529216\n[4200]\tvalid_0's SMAPE: 0.529211\n[4400]\tvalid_0's SMAPE: 0.529208\nEarly stopping, best iteration is:\n[4308]\tvalid_0's SMAPE: 0.529207\n Training seed 2023 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.559263\n[400]\tvalid_0's SMAPE: 0.541627\n[600]\tvalid_0's SMAPE: 0.53593\n[800]\tvalid_0's SMAPE: 0.533354\n[1000]\tvalid_0's SMAPE: 0.532102\n[1200]\tvalid_0's SMAPE: 0.531338\n[1400]\tvalid_0's SMAPE: 0.530963\n[1600]\tvalid_0's SMAPE: 0.530704\n[1800]\tvalid_0's SMAPE: 0.530566\n[2000]\tvalid_0's SMAPE: 0.530461\n[2200]\tvalid_0's SMAPE: 0.530391\n[2400]\tvalid_0's SMAPE: 0.530344\n[2600]\tvalid_0's SMAPE: 0.530295\n[2800]\tvalid_0's SMAPE: 0.530268\n[3000]\tvalid_0's SMAPE: 0.530242\n[3200]\tvalid_0's SMAPE: 0.530219\n[3400]\tvalid_0's SMAPE: 0.530203\n[3600]\tvalid_0's SMAPE: 0.530182\n[3800]\tvalid_0's SMAPE: 0.530171\n[4000]\tvalid_0's SMAPE: 0.530163\n[4200]\tvalid_0's SMAPE: 0.530156\n[4400]\tvalid_0's SMAPE: 0.53015\n[4600]\tvalid_0's SMAPE: 0.530143\n[4800]\tvalid_0's SMAPE: 0.530137\n[5000]\tvalid_0's SMAPE: 0.530136\nDid not meet early stopping. Best iteration is:\n[4998]\tvalid_0's SMAPE: 0.530136\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds:  60%|███████████████████████████                  | 3/5 [3:53:59<2:35:06, 4653.21s/it]","output_type":"stream"},{"name":"stdout","text":" Fold 3 SMAPE: 52.7756%\n\n--- Fold 4 ---\n Training seed 42 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.552103\n[400]\tvalid_0's SMAPE: 0.535477\n[600]\tvalid_0's SMAPE: 0.530069\n[800]\tvalid_0's SMAPE: 0.527488\n[1000]\tvalid_0's SMAPE: 0.526299\n[1200]\tvalid_0's SMAPE: 0.525596\n[1400]\tvalid_0's SMAPE: 0.525234\n[1600]\tvalid_0's SMAPE: 0.525026\n[1800]\tvalid_0's SMAPE: 0.524865\n[2000]\tvalid_0's SMAPE: 0.524784\n[2200]\tvalid_0's SMAPE: 0.524701\n[2400]\tvalid_0's SMAPE: 0.524651\n[2600]\tvalid_0's SMAPE: 0.524613\n[2800]\tvalid_0's SMAPE: 0.524586\n[3000]\tvalid_0's SMAPE: 0.524568\n[3200]\tvalid_0's SMAPE: 0.524558\n[3400]\tvalid_0's SMAPE: 0.52455\n[3600]\tvalid_0's SMAPE: 0.524542\n[3800]\tvalid_0's SMAPE: 0.524535\n[4000]\tvalid_0's SMAPE: 0.524527\n[4200]\tvalid_0's SMAPE: 0.524521\n[4400]\tvalid_0's SMAPE: 0.524517\n[4600]\tvalid_0's SMAPE: 0.524511\n[4800]\tvalid_0's SMAPE: 0.524509\n[5000]\tvalid_0's SMAPE: 0.524502\nDid not meet early stopping. Best iteration is:\n[4998]\tvalid_0's SMAPE: 0.524502\n Training seed 2023 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.553003\n[400]\tvalid_0's SMAPE: 0.535739\n[600]\tvalid_0's SMAPE: 0.530265\n[800]\tvalid_0's SMAPE: 0.527758\n[1000]\tvalid_0's SMAPE: 0.526485\n[1200]\tvalid_0's SMAPE: 0.525736\n[1400]\tvalid_0's SMAPE: 0.525403\n[1600]\tvalid_0's SMAPE: 0.525181\n[1800]\tvalid_0's SMAPE: 0.525004\n[2000]\tvalid_0's SMAPE: 0.524893\n[2200]\tvalid_0's SMAPE: 0.524832\n[2400]\tvalid_0's SMAPE: 0.524781\n[2600]\tvalid_0's SMAPE: 0.524757\n[2800]\tvalid_0's SMAPE: 0.524735\n[3000]\tvalid_0's SMAPE: 0.524717\n[3200]\tvalid_0's SMAPE: 0.524709\n[3400]\tvalid_0's SMAPE: 0.524691\n[3600]\tvalid_0's SMAPE: 0.52468\n[3800]\tvalid_0's SMAPE: 0.52467\n[4000]\tvalid_0's SMAPE: 0.52466\n[4200]\tvalid_0's SMAPE: 0.524653\n[4400]\tvalid_0's SMAPE: 0.524647\n[4600]\tvalid_0's SMAPE: 0.524643\n[4800]\tvalid_0's SMAPE: 0.524643\n[5000]\tvalid_0's SMAPE: 0.524641\nDid not meet early stopping. Best iteration is:\n[4941]\tvalid_0's SMAPE: 0.524641\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds:  80%|████████████████████████████████████         | 4/5 [5:15:06<1:18:57, 4737.91s/it]","output_type":"stream"},{"name":"stdout","text":" Fold 4 SMAPE: 52.2571%\n\n--- Fold 5 ---\n Training seed 42 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.557567\n[400]\tvalid_0's SMAPE: 0.54008\n[600]\tvalid_0's SMAPE: 0.534528\n[800]\tvalid_0's SMAPE: 0.531903\n[1000]\tvalid_0's SMAPE: 0.53068\n[1200]\tvalid_0's SMAPE: 0.530059\n[1400]\tvalid_0's SMAPE: 0.529731\n[1600]\tvalid_0's SMAPE: 0.529506\n[1800]\tvalid_0's SMAPE: 0.52934\n[2000]\tvalid_0's SMAPE: 0.529226\n[2200]\tvalid_0's SMAPE: 0.529154\n[2400]\tvalid_0's SMAPE: 0.529097\n[2600]\tvalid_0's SMAPE: 0.529047\n[2800]\tvalid_0's SMAPE: 0.529029\n[3000]\tvalid_0's SMAPE: 0.529008\n[3200]\tvalid_0's SMAPE: 0.528991\n[3400]\tvalid_0's SMAPE: 0.528974\n[3600]\tvalid_0's SMAPE: 0.528966\n[3800]\tvalid_0's SMAPE: 0.528961\n[4000]\tvalid_0's SMAPE: 0.528952\n[4200]\tvalid_0's SMAPE: 0.528947\n[4400]\tvalid_0's SMAPE: 0.528942\n[4600]\tvalid_0's SMAPE: 0.528935\n[4800]\tvalid_0's SMAPE: 0.52893\n[5000]\tvalid_0's SMAPE: 0.528927\nDid not meet early stopping. Best iteration is:\n[4995]\tvalid_0's SMAPE: 0.528927\n Training seed 2023 ...\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's SMAPE: 0.555909\n[400]\tvalid_0's SMAPE: 0.538077\n[600]\tvalid_0's SMAPE: 0.532392\n[800]\tvalid_0's SMAPE: 0.529951\n[1000]\tvalid_0's SMAPE: 0.52861\n[1200]\tvalid_0's SMAPE: 0.527967\n[1400]\tvalid_0's SMAPE: 0.527582\n[1600]\tvalid_0's SMAPE: 0.527315\n[1800]\tvalid_0's SMAPE: 0.527164\n[2000]\tvalid_0's SMAPE: 0.527034\n[2200]\tvalid_0's SMAPE: 0.526966\n[2400]\tvalid_0's SMAPE: 0.526916\n[2600]\tvalid_0's SMAPE: 0.526884\n[2800]\tvalid_0's SMAPE: 0.526862\n[3000]\tvalid_0's SMAPE: 0.526833\n[3200]\tvalid_0's SMAPE: 0.526806\n[3400]\tvalid_0's SMAPE: 0.526791\n[3600]\tvalid_0's SMAPE: 0.526776\n[3800]\tvalid_0's SMAPE: 0.526768\n[4000]\tvalid_0's SMAPE: 0.526766\n[4200]\tvalid_0's SMAPE: 0.526757\n[4400]\tvalid_0's SMAPE: 0.526753\n[4600]\tvalid_0's SMAPE: 0.526746\n[4800]\tvalid_0's SMAPE: 0.526742\n[5000]\tvalid_0's SMAPE: 0.526737\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's SMAPE: 0.526737\n","output_type":"stream"},{"name":"stderr","text":"LGB CV folds: 100%|███████████████████████████████████████████████| 5/5 [6:37:35<00:00, 4771.15s/it]","output_type":"stream"},{"name":"stdout","text":" Fold 5 SMAPE: 52.5952%\n\nCV OOF SMAPE: 52.6768%\nPer-fold SMAPEs: [53.16061619771864, 52.59566677781164, 52.77564688312958, 52.257131477181886, 52.595165530223106]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Device and multi-GPU support\ngpu_count = torch.cuda.device_count()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device, \"GPU count:\", gpu_count)\n\n# Simple MLP class\nclass MLPreg(nn.Module):\n    def __init__(self, in_dim, hidden=[1024,512,256], dropout=0.3):\n        super().__init__()\n        layers = []\n        prev = in_dim\n        for h in hidden:\n            layers.append(nn.Linear(prev, h))\n            layers.append(nn.BatchNorm1d(h))\n            layers.append(nn.ReLU(inplace=True))\n            layers.append(nn.Dropout(dropout))\n            prev = h\n        layers.append(nn.Linear(prev, 1))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n# Training util\ndef train_mlp_one_fold(X_tr, y_tr_log, X_val, y_val_log, in_dim, seed=42,\n                       epochs=30, batch_size=1024, lr=1e-3, patience=5):\n    torch.manual_seed(seed)\n    model = MLPreg(in_dim).to(device)\n    if gpu_count > 1:\n        model = nn.DataParallel(model)\n    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    criterion = nn.L1Loss()  # MAE on log scale\n    train_ds = TensorDataset(torch.from_numpy(X_tr.astype(np.float32)), torch.from_numpy(y_tr_log.astype(np.float32)))\n    val_ds   = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val_log.astype(np.float32)))\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    best_val = 1e9; best_epoch = -1; best_state=None; wait=0\n    for ep in range(epochs):\n        model.train()\n        train_loss = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device); yb = yb.to(device)\n            optim.zero_grad()\n            pred = model(xb)\n            loss = criterion(pred, yb)\n            loss.backward()\n            optim.step()\n            train_loss += loss.item() * xb.size(0)\n        train_loss /= len(train_loader.dataset)\n        # val\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device); yb = yb.to(device)\n                pred = model(xb)\n                loss = criterion(pred, yb)\n                val_loss += loss.item() * xb.size(0)\n        val_loss /= len(val_loader.dataset)\n        # print progress\n        print(f\" Epoch {ep+1}/{epochs} | train MAE(log): {train_loss:.6f} | val MAE(log): {val_loss:.6f}\")\n        # early stop\n        if val_loss < best_val - 1e-6:\n            best_val = val_loss; best_epoch = ep; best_state = {k:v.cpu() for k,v in model.state_dict().items()}; wait=0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\" Early stopping triggered.\")\n                break\n    # load best\n    model.load_state_dict(best_state)\n    # predict\n    def predict_np(X):\n        model.eval()\n        preds = []\n        bs = 1024\n        with torch.no_grad():\n            for i in range(0, X.shape[0], bs):\n                xb = torch.from_numpy(X[i:i+bs].astype(np.float32)).to(device)\n                preds.append(model(xb).detach().cpu().numpy())\n        return np.concatenate(preds).reshape(-1)\n    return model, predict_np\n\n# KFold for MLP\nfrom sklearn.model_selection import KFold\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\noof_pred_mlp_log = np.zeros(len(train))\npred_test_mlp_log = np.zeros(len(test))\npred_sample_mlp_log = np.zeros(len(sample_test))\n\nfold = 0\nin_dim = X_train.shape[1]\nfor tr_idx, val_idx in tqdm(kf.split(X_train), total=n_splits, desc='MLP CV folds', ncols=120):\n    fold+=1\n    print(f\"\\nMLP Fold {fold}\")\n    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n    y_tr_log, y_val_log = y_log[tr_idx], y_log[val_idx]\n    model, pred_fn = train_mlp_one_fold(X_tr, y_tr_log, X_val, y_val_log,\n                                        in_dim=in_dim, seed=42+fold, epochs=50, batch_size=2048, lr=1e-3, patience=6)\n    # predict\n    oof_pred_mlp_log[val_idx] = pred_fn(X_val)\n    pred_test_mlp_log += pred_fn(X_test) / n_splits\n    pred_sample_mlp_log += pred_fn(X_sample) / n_splits\n    # compute fold smape (on orig scale)\n    fold_smape = smape(np.expm1(y_val_log), np.expm1(oof_pred_mlp_log[val_idx]))\n    print(f\" MLP Fold {fold} SMAPE: {fold_smape:.4f}%\")\n    # free GPU memory if possible\n    del model; gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\noof_pred_mlp = np.expm1(oof_pred_mlp_log)\npred_test_mlp = np.expm1(pred_test_mlp_log)\npred_sample_mlp = np.expm1(pred_sample_mlp_log)\n\nprint(\"MLP CV OOF SMAPE:\", smape(y, oof_pred_mlp))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:48:30.329320Z","iopub.execute_input":"2025-10-13T16:48:30.329598Z","iopub.status.idle":"2025-10-13T16:51:56.781189Z","shell.execute_reply.started":"2025-10-13T16:48:30.329577Z","shell.execute_reply":"2025-10-13T16:51:56.780475Z"}},"outputs":[{"name":"stdout","text":"Device: cuda GPU count: 2\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds:   0%|                                                                               | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nMLP Fold 1\n Epoch 1/50 | train MAE(log): 1.384583 | val MAE(log): 1.198231\n Epoch 2/50 | train MAE(log): 0.676849 | val MAE(log): 0.590060\n Epoch 3/50 | train MAE(log): 0.620285 | val MAE(log): 0.604934\n Epoch 4/50 | train MAE(log): 0.595244 | val MAE(log): 0.581982\n Epoch 5/50 | train MAE(log): 0.574793 | val MAE(log): 0.565367\n Epoch 6/50 | train MAE(log): 0.559439 | val MAE(log): 0.565507\n Epoch 7/50 | train MAE(log): 0.541983 | val MAE(log): 0.558111\n Epoch 8/50 | train MAE(log): 0.530541 | val MAE(log): 0.562904\n Epoch 9/50 | train MAE(log): 0.516340 | val MAE(log): 0.544009\n Epoch 10/50 | train MAE(log): 0.505742 | val MAE(log): 0.540821\n Epoch 11/50 | train MAE(log): 0.491636 | val MAE(log): 0.546589\n Epoch 12/50 | train MAE(log): 0.482345 | val MAE(log): 0.546942\n Epoch 13/50 | train MAE(log): 0.472169 | val MAE(log): 0.541584\n Epoch 14/50 | train MAE(log): 0.462495 | val MAE(log): 0.542608\n Epoch 15/50 | train MAE(log): 0.452763 | val MAE(log): 0.537582\n Epoch 16/50 | train MAE(log): 0.446291 | val MAE(log): 0.537413\n Epoch 17/50 | train MAE(log): 0.436877 | val MAE(log): 0.540366\n Epoch 18/50 | train MAE(log): 0.432136 | val MAE(log): 0.534384\n Epoch 19/50 | train MAE(log): 0.423142 | val MAE(log): 0.535025\n Epoch 20/50 | train MAE(log): 0.415637 | val MAE(log): 0.532130\n Epoch 21/50 | train MAE(log): 0.409824 | val MAE(log): 0.542671\n Epoch 22/50 | train MAE(log): 0.405432 | val MAE(log): 0.531211\n Epoch 23/50 | train MAE(log): 0.399153 | val MAE(log): 0.532850\n Epoch 24/50 | train MAE(log): 0.392090 | val MAE(log): 0.528281\n Epoch 25/50 | train MAE(log): 0.387457 | val MAE(log): 0.533785\n Epoch 26/50 | train MAE(log): 0.380847 | val MAE(log): 0.526498\n Epoch 27/50 | train MAE(log): 0.378996 | val MAE(log): 0.530742\n Epoch 28/50 | train MAE(log): 0.371929 | val MAE(log): 0.531011\n Epoch 29/50 | train MAE(log): 0.369212 | val MAE(log): 0.528892\n Epoch 30/50 | train MAE(log): 0.364351 | val MAE(log): 0.528288\n Epoch 31/50 | train MAE(log): 0.359486 | val MAE(log): 0.532971\n Epoch 32/50 | train MAE(log): 0.358073 | val MAE(log): 0.527809\n Early stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds:  20%|██████████████▏                                                        | 1/5 [00:54<03:38, 54.69s/it]","output_type":"stream"},{"name":"stdout","text":" MLP Fold 1 SMAPE: 52.3712%\n\nMLP Fold 2\n Epoch 1/50 | train MAE(log): 1.348852 | val MAE(log): 1.070431\n Epoch 2/50 | train MAE(log): 0.676784 | val MAE(log): 0.586850\n Epoch 3/50 | train MAE(log): 0.622875 | val MAE(log): 0.592987\n Epoch 4/50 | train MAE(log): 0.593306 | val MAE(log): 0.559240\n Epoch 5/50 | train MAE(log): 0.576215 | val MAE(log): 0.548739\n Epoch 6/50 | train MAE(log): 0.556954 | val MAE(log): 0.550796\n Epoch 7/50 | train MAE(log): 0.543279 | val MAE(log): 0.544451\n Epoch 8/50 | train MAE(log): 0.527679 | val MAE(log): 0.550272\n Epoch 9/50 | train MAE(log): 0.514044 | val MAE(log): 0.538024\n Epoch 10/50 | train MAE(log): 0.504791 | val MAE(log): 0.531981\n Epoch 11/50 | train MAE(log): 0.491958 | val MAE(log): 0.536093\n Epoch 12/50 | train MAE(log): 0.482653 | val MAE(log): 0.535767\n Epoch 13/50 | train MAE(log): 0.468815 | val MAE(log): 0.525764\n Epoch 14/50 | train MAE(log): 0.463251 | val MAE(log): 0.529534\n Epoch 15/50 | train MAE(log): 0.452081 | val MAE(log): 0.532098\n Epoch 16/50 | train MAE(log): 0.445066 | val MAE(log): 0.526485\n Epoch 17/50 | train MAE(log): 0.437948 | val MAE(log): 0.527500\n Epoch 18/50 | train MAE(log): 0.428201 | val MAE(log): 0.524103\n Epoch 19/50 | train MAE(log): 0.420205 | val MAE(log): 0.528799\n Epoch 20/50 | train MAE(log): 0.417455 | val MAE(log): 0.534827\n Epoch 21/50 | train MAE(log): 0.410196 | val MAE(log): 0.535024\n Epoch 22/50 | train MAE(log): 0.403140 | val MAE(log): 0.527105\n Epoch 23/50 | train MAE(log): 0.395126 | val MAE(log): 0.524386\n Epoch 24/50 | train MAE(log): 0.390034 | val MAE(log): 0.523038\n Epoch 25/50 | train MAE(log): 0.384149 | val MAE(log): 0.525869\n Epoch 26/50 | train MAE(log): 0.378030 | val MAE(log): 0.525822\n Epoch 27/50 | train MAE(log): 0.374888 | val MAE(log): 0.522456\n Epoch 28/50 | train MAE(log): 0.371464 | val MAE(log): 0.522110\n Epoch 29/50 | train MAE(log): 0.364629 | val MAE(log): 0.521126\n Epoch 30/50 | train MAE(log): 0.362606 | val MAE(log): 0.521733\n Epoch 31/50 | train MAE(log): 0.357600 | val MAE(log): 0.521189\n Epoch 32/50 | train MAE(log): 0.354612 | val MAE(log): 0.524984\n Epoch 33/50 | train MAE(log): 0.348621 | val MAE(log): 0.521329\n Epoch 34/50 | train MAE(log): 0.348114 | val MAE(log): 0.523805\n Epoch 35/50 | train MAE(log): 0.342758 | val MAE(log): 0.518818\n Epoch 36/50 | train MAE(log): 0.338125 | val MAE(log): 0.518818\n Epoch 37/50 | train MAE(log): 0.335278 | val MAE(log): 0.517873\n Epoch 38/50 | train MAE(log): 0.333725 | val MAE(log): 0.519582\n Epoch 39/50 | train MAE(log): 0.330838 | val MAE(log): 0.521604\n Epoch 40/50 | train MAE(log): 0.329639 | val MAE(log): 0.519075\n Epoch 41/50 | train MAE(log): 0.326653 | val MAE(log): 0.522542\n Epoch 42/50 | train MAE(log): 0.323395 | val MAE(log): 0.515693\n Epoch 43/50 | train MAE(log): 0.320696 | val MAE(log): 0.514964\n Epoch 44/50 | train MAE(log): 0.317915 | val MAE(log): 0.521560\n Epoch 45/50 | train MAE(log): 0.313720 | val MAE(log): 0.519670\n Epoch 46/50 | train MAE(log): 0.309129 | val MAE(log): 0.516458\n Epoch 47/50 | train MAE(log): 0.311817 | val MAE(log): 0.523110\n Epoch 48/50 | train MAE(log): 0.308385 | val MAE(log): 0.533359\n Epoch 49/50 | train MAE(log): 0.305455 | val MAE(log): 0.515712\n Early stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds:  40%|████████████████████████████▍                                          | 2/5 [01:53<02:50, 56.91s/it]","output_type":"stream"},{"name":"stdout","text":" MLP Fold 2 SMAPE: 51.4164%\n\nMLP Fold 3\n Epoch 1/50 | train MAE(log): 1.416945 | val MAE(log): 1.179944\n Epoch 2/50 | train MAE(log): 0.672823 | val MAE(log): 0.611814\n Epoch 3/50 | train MAE(log): 0.614358 | val MAE(log): 0.568079\n Epoch 4/50 | train MAE(log): 0.590684 | val MAE(log): 0.551207\n Epoch 5/50 | train MAE(log): 0.573918 | val MAE(log): 0.549584\n Epoch 6/50 | train MAE(log): 0.556973 | val MAE(log): 0.559423\n Epoch 7/50 | train MAE(log): 0.540290 | val MAE(log): 0.539862\n Epoch 8/50 | train MAE(log): 0.524483 | val MAE(log): 0.543905\n Epoch 9/50 | train MAE(log): 0.509255 | val MAE(log): 0.545150\n Epoch 10/50 | train MAE(log): 0.500002 | val MAE(log): 0.537605\n Epoch 11/50 | train MAE(log): 0.492756 | val MAE(log): 0.538928\n Epoch 12/50 | train MAE(log): 0.480973 | val MAE(log): 0.538684\n Epoch 13/50 | train MAE(log): 0.471932 | val MAE(log): 0.532348\n Epoch 14/50 | train MAE(log): 0.461839 | val MAE(log): 0.527084\n Epoch 15/50 | train MAE(log): 0.452762 | val MAE(log): 0.534428\n Epoch 16/50 | train MAE(log): 0.443419 | val MAE(log): 0.540157\n Epoch 17/50 | train MAE(log): 0.439583 | val MAE(log): 0.528265\n Epoch 18/50 | train MAE(log): 0.427284 | val MAE(log): 0.528635\n Epoch 19/50 | train MAE(log): 0.421302 | val MAE(log): 0.528230\n Epoch 20/50 | train MAE(log): 0.412451 | val MAE(log): 0.527418\n Early stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds:  60%|██████████████████████████████████████████▌                            | 3/5 [02:17<01:24, 42.06s/it]","output_type":"stream"},{"name":"stdout","text":" MLP Fold 3 SMAPE: 52.7250%\n\nMLP Fold 4\n Epoch 1/50 | train MAE(log): 1.425834 | val MAE(log): 1.285322\n Epoch 2/50 | train MAE(log): 0.661848 | val MAE(log): 0.627251\n Epoch 3/50 | train MAE(log): 0.614595 | val MAE(log): 0.599901\n Epoch 4/50 | train MAE(log): 0.591679 | val MAE(log): 0.557321\n Epoch 5/50 | train MAE(log): 0.569837 | val MAE(log): 0.563097\n Epoch 6/50 | train MAE(log): 0.554981 | val MAE(log): 0.548382\n Epoch 7/50 | train MAE(log): 0.540622 | val MAE(log): 0.544144\n Epoch 8/50 | train MAE(log): 0.528057 | val MAE(log): 0.541986\n Epoch 9/50 | train MAE(log): 0.511815 | val MAE(log): 0.544009\n Epoch 10/50 | train MAE(log): 0.500247 | val MAE(log): 0.527734\n Epoch 11/50 | train MAE(log): 0.490283 | val MAE(log): 0.530020\n Epoch 12/50 | train MAE(log): 0.478578 | val MAE(log): 0.535270\n Epoch 13/50 | train MAE(log): 0.467935 | val MAE(log): 0.516675\n Epoch 14/50 | train MAE(log): 0.462735 | val MAE(log): 0.519130\n Epoch 15/50 | train MAE(log): 0.451136 | val MAE(log): 0.524334\n Epoch 16/50 | train MAE(log): 0.442120 | val MAE(log): 0.520720\n Epoch 17/50 | train MAE(log): 0.435520 | val MAE(log): 0.526050\n Epoch 18/50 | train MAE(log): 0.426740 | val MAE(log): 0.519720\n Epoch 19/50 | train MAE(log): 0.420770 | val MAE(log): 0.526885\n Early stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds:  80%|████████████████████████████████████████████████████████▊              | 4/5 [02:41<00:34, 34.72s/it]","output_type":"stream"},{"name":"stdout","text":" MLP Fold 4 SMAPE: 51.6580%\n\nMLP Fold 5\n Epoch 1/50 | train MAE(log): 1.386627 | val MAE(log): 1.055416\n Epoch 2/50 | train MAE(log): 0.678534 | val MAE(log): 0.591036\n Epoch 3/50 | train MAE(log): 0.626863 | val MAE(log): 0.581930\n Epoch 4/50 | train MAE(log): 0.601963 | val MAE(log): 0.562463\n Epoch 5/50 | train MAE(log): 0.579324 | val MAE(log): 0.551381\n Epoch 6/50 | train MAE(log): 0.560079 | val MAE(log): 0.566916\n Epoch 7/50 | train MAE(log): 0.548390 | val MAE(log): 0.543303\n Epoch 8/50 | train MAE(log): 0.533517 | val MAE(log): 0.544364\n Epoch 9/50 | train MAE(log): 0.520534 | val MAE(log): 0.539819\n Epoch 10/50 | train MAE(log): 0.508407 | val MAE(log): 0.551492\n Epoch 11/50 | train MAE(log): 0.496616 | val MAE(log): 0.544622\n Epoch 12/50 | train MAE(log): 0.482742 | val MAE(log): 0.533874\n Epoch 13/50 | train MAE(log): 0.473548 | val MAE(log): 0.536029\n Epoch 14/50 | train MAE(log): 0.464694 | val MAE(log): 0.544246\n Epoch 15/50 | train MAE(log): 0.453943 | val MAE(log): 0.534028\n Epoch 16/50 | train MAE(log): 0.448058 | val MAE(log): 0.533031\n Epoch 17/50 | train MAE(log): 0.439761 | val MAE(log): 0.527328\n Epoch 18/50 | train MAE(log): 0.432755 | val MAE(log): 0.527834\n Epoch 19/50 | train MAE(log): 0.424168 | val MAE(log): 0.526112\n Epoch 20/50 | train MAE(log): 0.418255 | val MAE(log): 0.530981\n Epoch 21/50 | train MAE(log): 0.412120 | val MAE(log): 0.529024\n Epoch 22/50 | train MAE(log): 0.405193 | val MAE(log): 0.526165\n Epoch 23/50 | train MAE(log): 0.401370 | val MAE(log): 0.523732\n Epoch 24/50 | train MAE(log): 0.396603 | val MAE(log): 0.530528\n Epoch 25/50 | train MAE(log): 0.386544 | val MAE(log): 0.523354\n Epoch 26/50 | train MAE(log): 0.384420 | val MAE(log): 0.533398\n Epoch 27/50 | train MAE(log): 0.378108 | val MAE(log): 0.527663\n Epoch 28/50 | train MAE(log): 0.374649 | val MAE(log): 0.524365\n Epoch 29/50 | train MAE(log): 0.370428 | val MAE(log): 0.521698\n Epoch 30/50 | train MAE(log): 0.365278 | val MAE(log): 0.523887\n Epoch 31/50 | train MAE(log): 0.363114 | val MAE(log): 0.527238\n Epoch 32/50 | train MAE(log): 0.359079 | val MAE(log): 0.524492\n Epoch 33/50 | train MAE(log): 0.352989 | val MAE(log): 0.526835\n Epoch 34/50 | train MAE(log): 0.352130 | val MAE(log): 0.523904\n Epoch 35/50 | train MAE(log): 0.346681 | val MAE(log): 0.526471\n Early stopping triggered.\n","output_type":"stream"},{"name":"stderr","text":"MLP CV folds: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [03:23<00:00, 40.62s/it]","output_type":"stream"},{"name":"stdout","text":" MLP Fold 5 SMAPE: 51.9865%\nMLP CV OOF SMAPE: 52.03141081160807\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# OOF arrays currently: oof_pred_lgb (from LGB), oof_pred_mlp\n# But ensure they are defined:\nassert 'oof_pred_lgb' in globals() or 'oof_pred_lgb' in locals()\n# oof_pred_lgb computed earlier, oof_pred_mlp computed from MLP\n\n# Grid search blending weight for LGB (w) and MLP (1-w)\nbest_w = None\nbest_score = 1e9\nfor w in tqdm(np.linspace(0,1,101), desc='blend grid'):\n    pred = w * oof_pred_lgb + (1-w) * oof_pred_mlp\n    score = smape(y, pred)\n    if score < best_score:\n        best_score = score\n        best_w = w\nprint(f\"Best blend weight for LGB = {best_w:.2f}, CV SMAPE = {best_score:.4f}%\")\n\n# Compose test & sample preds\npred_test_blend = best_w * pred_test_lgb + (1-best_w) * pred_test_mlp\npred_sample_blend = best_w * pred_sample_lgb + (1-best_w) * pred_sample_mlp\n\n# Clip to positive values\npred_test_blend = np.clip(pred_test_blend, 0.01, None)\npred_sample_blend = np.clip(pred_sample_blend, 0.01, None)\n\nprint(\"Final blended CV SMAPE (on train OOF):\", best_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:56:04.426280Z","iopub.execute_input":"2025-10-13T16:56:04.426962Z","iopub.status.idle":"2025-10-13T16:56:04.444191Z","shell.execute_reply.started":"2025-10-13T16:56:04.426938Z","shell.execute_reply":"2025-10-13T16:56:04.443248Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/38683511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# OOF arrays currently: oof_pred_lgb (from LGB), oof_pred_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# But ensure they are defined:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'oof_pred_lgb'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'oof_pred_lgb'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# oof_pred_lgb computed earlier, oof_pred_mlp computed from MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "],"ename":"AssertionError","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# Correcting errors\n# Robust blending helper: find/load OOF + test preds for LGB and MLP, then blend\nimport os, sys\nimport numpy as np\nfrom tqdm import tqdm\n\nWORK_DIR = '/kaggle/working' if 'KAGGLE_WORKING_DIR' not in globals() else KAGGLE_WORKING_DIR\n# list of candidate variable names in memory for each required array\ncands = {\n    'oof_lgb_orig': ['oof_pred_lgb', 'oof_pred_lgb_orig', 'oof_pred_lgb_final', 'oof_pred_lgb_final_orig'],\n    'oof_lgb_log' : ['oof_pred_log', 'preds_oof_log', 'preds_oof_log', 'oof_pred_log'],\n    'pred_test_lgb_orig': ['pred_test_lgb', 'pred_test_lgb_orig', 'pred_test_lgb_final'],\n    'pred_test_lgb_log' : ['pred_test_log', 'preds_test_log', 'pred_test_log'],\n    'oof_mlp_orig' : ['oof_pred_mlp', 'oof_pred_mlp_orig', 'oof_pred_mlp_final'],\n    'oof_mlp_log'  : ['oof_pred_mlp_log', 'oof_pred_mlp_log_final'],\n    'pred_test_mlp_orig': ['pred_test_mlp', 'pred_test_mlp_orig', 'pred_test_mlp_final'],\n    'pred_test_mlp_log' : ['pred_test_mlp_log', 'pred_test_mlp_log_final']\n}\n\n# candidate filenames to try loading from WORK_DIR\nfile_cands = {\n    'oof_lgb_orig': [os.path.join(WORK_DIR, 'oof_pred_lgb.npy'), os.path.join(WORK_DIR, 'oof_pred_lgb_orig.npy'), os.path.join(WORK_DIR, 'oof_pred_lgb_final.npy')],\n    'oof_lgb_log' : [os.path.join(WORK_DIR, 'oof_pred_log.npy'), os.path.join(WORK_DIR, 'preds_oof_log.npy')],\n    'pred_test_lgb_orig': [os.path.join(WORK_DIR, 'pred_test_lgb.npy'), os.path.join(WORK_DIR, 'pred_test_lgb_orig.npy')],\n    'pred_test_lgb_log' : [os.path.join(WORK_DIR, 'pred_test_log.npy'), os.path.join(WORK_DIR, 'preds_test_log.npy')],\n    'oof_mlp_orig' : [os.path.join(WORK_DIR, 'oof_pred_mlp.npy'), os.path.join(WORK_DIR, 'oof_pred_mlp_orig.npy')],\n    'oof_mlp_log'  : [os.path.join(WORK_DIR, 'oof_pred_mlp_log.npy')],\n    'pred_test_mlp_orig': [os.path.join(WORK_DIR, 'pred_test_mlp.npy')],\n    'pred_test_mlp_log' : [os.path.join(WORK_DIR, 'pred_test_mlp_log.npy')]\n}\n\n# utility to find variable in memory\ndef find_in_memory(names):\n    for n in names:\n        if n in globals():\n            return globals()[n]\n        if n in locals():\n            return locals()[n]\n    return None\n\n# utility to try load npy files\ndef try_load(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            try:\n                arr = np.load(p)\n                print(f\"Loaded {p} shape={arr.shape}\")\n                return arr\n            except Exception as e:\n                print(f\"Failed loading {p}: {e}\")\n    return None\n\n# Attempt to populate arrays (original scale)\ndef get_array(key):\n    # try orig variables in memory\n    arr = find_in_memory(cands.get(key, []))\n    if arr is not None:\n        print(f\"Found {key} in memory (orig) shape={np.array(arr).shape}\")\n        return np.array(arr)\n    # try log-named in memory and convert if found\n    log_key = key.replace('_orig','_log')\n    arr_log = find_in_memory(cands.get(log_key, []))\n    if arr_log is not None:\n        arr_log = np.array(arr_log)\n        print(f\"Found {log_key} in memory shape={arr_log.shape} -> converting expm1\")\n        return np.expm1(arr_log)\n    # try file loads (orig)\n    arr = try_load(file_cands.get(key, []))\n    if arr is not None:\n        return arr\n    # try file loads (log) and convert\n    arr_log = try_load(file_cands.get(log_key, []))\n    if arr_log is not None:\n        try:\n            return np.expm1(arr_log)\n        except:\n            return None\n    # nothing found\n    return None\n\n# fetch arrays\noof_lgb = get_array('oof_lgb_orig')\npred_test_lgb = get_array('pred_test_lgb_orig')\noof_mlp = get_array('oof_mlp_orig')\npred_test_mlp = get_array('pred_test_mlp_orig')\n\n# Try alternative names if above failed (older notebook variations)\nif oof_lgb is None:\n    # try older variable preds_oof_log -> expm1\n    if 'preds_oof_log' in globals():\n        oof_lgb = np.expm1(np.array(globals()['preds_oof_log']))\n        print(\"Recovered oof_lgb from preds_oof_log (converted expm1).\")\nif oof_mlp is None:\n    if 'oof_pred_mlp' in globals():\n        oof_mlp = np.array(globals()['oof_pred_mlp'])\n        print(\"Recovered oof_mlp from oof_pred_mlp in memory.\")\n\n# sanity checks: lengths\nmissing = []\nif oof_lgb is None:\n    missing.append('oof_pred_lgb (OOF from LightGBM)')\nif oof_mlp is None:\n    missing.append('oof_pred_mlp (OOF from MLP)')\nif pred_test_lgb is None:\n    missing.append('pred_test_lgb (test preds from LightGBM)')\nif pred_test_mlp is None:\n    missing.append('pred_test_mlp (test preds from MLP)')\n\nif missing:\n    print(\"Could not find all required arrays. Missing:\", missing)\n    print(\"Possible fixes:\")\n    print(\" - Re-run the LightGBM training cell to recreate oof_pred_lgb / pred_test_lgb,\")\n    print(\" - Re-run the MLP training cell to recreate oof_pred_mlp / pred_test_mlp,\")\n    print(\" - Or load saved .npy files (e.g., np.load('/kaggle/working/oof_pred_lgb.npy')) into memory before blending.\")\n    raise RuntimeError(\"Required arrays missing. See message above.\")\n\n# Final shape checks and proceed with blending\nprint(\"Shapes: oof_lgb:\", oof_lgb.shape, \"oof_mlp:\", oof_mlp.shape,\n      \"pred_test_lgb:\", pred_test_lgb.shape, \"pred_test_mlp:\", pred_test_mlp.shape)\n\n# If sample predictions exist load them too (optional)\n# We'll attempt to find sample preds similarly, but not required for blending weights.\n# Compute best blend weight on OOF by grid search\nbest_w = None\nbest_score = 1e9\nfor w in tqdm(np.linspace(0,1,101), desc='blend grid', ncols=100):\n    blended = w * oof_lgb + (1.0 - w) * oof_mlp\n    score = smape(y, blended)   # assumes y (train prices) present in memory\n    if score < best_score:\n        best_score = score\n        best_w = w\n\nprint(f\"Best blend weight for LGB = {best_w:.2f}, CV SMAPE = {best_score:.4f}%\")\n\n# Compose test predictions\npred_test_blend = best_w * pred_test_lgb + (1.0 - best_w) * pred_test_mlp\npred_test_blend = np.clip(pred_test_blend, 0.01, None)\n\n# Save final blended predictions (change path if needed)\nout_path = os.path.join(WORK_DIR, 'test_out_blended.csv')\nimport pandas as pd\nif 'test' in globals():\n    out_df = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test_blend})\n    out_df.to_csv(out_path, index=False)\n    print(\"Saved blended test predictions to:\", out_path)\nelse:\n    print(\"Warning: 'test' dataframe not in memory; not saving CSV. You can save pred_test_blend manually.\")\n\n# If sample predictions exist similarly attempt to blend and save\n# Try to fetch sample preds (optional)\ndef try_get_sample_preds():\n    for name in ['pred_sample_lgb', 'pred_sample_lgb_orig', 'pred_sample_lgb_final', 'pred_sample_lgb.npy']:\n        if name in globals(): return globals()[name]\n    for p in [os.path.join(WORK_DIR, 'pred_sample_lgb.npy'), os.path.join(WORK_DIR,'pred_sample_blend.npy')]:\n        if os.path.exists(p):\n            return np.load(p)\n    return None\n\n# End\nprint(\"Blending done. CV blended SMAPE:\", best_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:57:13.441950Z","iopub.execute_input":"2025-10-13T16:57:13.442702Z","iopub.status.idle":"2025-10-13T16:57:13.715390Z","shell.execute_reply.started":"2025-10-13T16:57:13.442676Z","shell.execute_reply":"2025-10-13T16:57:13.714816Z"}},"outputs":[{"name":"stdout","text":"Found oof_lgb_log in memory shape=(75000,) -> converting expm1\nFound pred_test_lgb_log in memory shape=(75000,) -> converting expm1\nFound oof_mlp_orig in memory (orig) shape=(75000,)\nFound pred_test_mlp_orig in memory (orig) shape=(75000,)\nShapes: oof_lgb: (75000,) oof_mlp: (75000,) pred_test_lgb: (75000,) pred_test_mlp: (75000,)\n","output_type":"stream"},{"name":"stderr","text":"blend grid: 100%|███████████████████████████████████████████████| 101/101 [00:00<00:00, 1024.51it/s]","output_type":"stream"},{"name":"stdout","text":"Best blend weight for LGB = 0.01, CV SMAPE = 52.0207%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved blended test predictions to: /kaggle/working/test_out_blended.csv\nBlending done. CV blended SMAPE: 52.02073513764387\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Again correcting errors\n# Robust saver: create pred_sample_blend if missing, then save test_out and sample_test_out\nimport os, numpy as np, pandas as pd\nWORK_DIR = '/kaggle/working' if 'KAGGLE_WORKING_DIR' not in globals() else KAGGLE_WORKING_DIR\n\ndef find_in_memory(names):\n    for n in names:\n        if n in globals():\n            return globals()[n]\n    return None\n\ndef try_load(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            try:\n                arr = np.load(p)\n                print(f\"Loaded {p} shape={arr.shape}\")\n                return arr\n            except Exception as e:\n                print(f\"Failed loading {p}: {e}\")\n    return None\n\n# 1) If pred_sample_blend exists, use it\npred_sample_blend = find_in_memory(['pred_sample_blend','pred_sample_blend_final','pred_sample_blend.npy'])\nif pred_sample_blend is not None:\n    print(\"Using pred_sample_blend from memory.\")\nelse:\n    print(\"pred_sample_blend not found in memory. Attempting to construct it...\")\n\n    # 2) Try to find pred_sample_lgb and pred_sample_mlp\n    cand_lgb = find_in_memory(['pred_sample_lgb','pred_sample_lgb_orig','pred_sample_lgb_final','pred_sample_lgb.npy'])\n    if cand_lgb is None:\n        cand_lgb = try_load([os.path.join(WORK_DIR,'pred_sample_lgb.npy'), os.path.join(WORK_DIR,'pred_sample_lgb_orig.npy'), os.path.join(WORK_DIR,'pred_sample_blend.npy')])\n    if cand_lgb is not None:\n        pred_sample_lgb = np.array(cand_lgb)\n        print(\"Found pred_sample_lgb shape:\", pred_sample_lgb.shape)\n    else:\n        pred_sample_lgb = None\n        print(\"pred_sample_lgb not found.\")\n\n    cand_mlp = find_in_memory(['pred_sample_mlp','pred_sample_mlp_orig','pred_sample_mlp.npy'])\n    if cand_mlp is None:\n        cand_mlp = try_load([os.path.join(WORK_DIR,'pred_sample_mlp.npy')])\n    if cand_mlp is not None:\n        pred_sample_mlp = np.array(cand_mlp)\n        print(\"Found pred_sample_mlp shape:\", pred_sample_mlp.shape)\n    else:\n        pred_sample_mlp = None\n        print(\"pred_sample_mlp not found.\")\n\n    # 3) Determine best_w (blend weight). Prefer existing best_w, otherwise compute from OOFs, otherwise default 0.5\n    best_w_use = None\n    if 'best_w' in globals():\n        best_w_use = globals()['best_w']\n        print(\"Using best_w from memory:\", best_w_use)\n    else:\n        # try to compute best_w from OOF arrays if available\n        oof_lgb = find_in_memory(['oof_pred_lgb','oof_pred_lgb_orig','oof_pred_lgb_final'])\n        if oof_lgb is None:\n            # try to load saved npy\n            oof_lgb = try_load([os.path.join(WORK_DIR,'oof_pred_lgb.npy'), os.path.join(WORK_DIR,'oof_pred_lgb_orig.npy')])\n        oof_mlp = find_in_memory(['oof_pred_mlp','oof_pred_mlp_orig'])\n        if oof_mlp is None:\n            oof_mlp = try_load([os.path.join(WORK_DIR,'oof_pred_mlp.npy')])\n\n        if (oof_lgb is not None) and (oof_mlp is not None) and ('y' in globals()):\n            oof_lgb = np.array(oof_lgb).reshape(-1)\n            oof_mlp = np.array(oof_mlp).reshape(-1)\n            y_arr = np.array(globals()['y']).reshape(-1)\n            # grid search small set for best_w\n            best_w_use = 0.0\n            best_score = 1e9\n            for w in np.linspace(0,1,101):\n                blended = w * oof_lgb + (1 - w) * oof_mlp\n                score = smape(y_arr, blended)\n                if score < best_score:\n                    best_score = score\n                    best_w_use = w\n            print(f\"Computed best_w from OOFs: {best_w_use:.2f} (OOF SMAPE {best_score:.4f}%)\")\n        else:\n            best_w_use = 0.5\n            print(\"Could not compute best_w from OOFs. Falling back to best_w = 0.5 (equal blend).\")\n\n    # 4) Construct pred_sample_blend using available preds\n    if (pred_sample_lgb is not None) and (pred_sample_mlp is not None):\n        pred_sample_blend = best_w_use * pred_sample_lgb + (1 - best_w_use) * pred_sample_mlp\n        print(\"Constructed pred_sample_blend by blending LGB and MLP sample preds.\")\n    elif (pred_sample_lgb is not None):\n        pred_sample_blend = pred_sample_lgb\n        print(\"Only LGB sample preds found; using those as pred_sample_blend.\")\n    elif (pred_sample_mlp is not None):\n        pred_sample_blend = pred_sample_mlp\n        print(\"Only MLP sample preds found; using those as pred_sample_blend.\")\n    else:\n        # 5) FALLBACK: use group median per unit_extracted or global median\n        print(\"No model sample predictions found. Using fallback: group median or global median.\")\n        # compute group median from train if possible\n        if 'train' in globals() and 'sample_test' in globals() and 'price' in train.columns:\n            try:\n                grp_med = train.groupby('unit_extracted')['price'].median().to_dict()\n                default_med = train['price'].median()\n                sample_units = sample_test['unit_extracted'].fillna('').map(lambda x: x if x!='' else None)\n                pred_sample_blend = []\n                for u in sample_test['unit_extracted'].fillna(''):\n                    val = grp_med.get(u, default_med)\n                    pred_sample_blend.append(val)\n                pred_sample_blend = np.array(pred_sample_blend)\n                print(\"Fallback: used unit_extracted group medians (with global median fallback).\")\n            except Exception as e:\n                print(\"Group median fallback failed:\", e)\n                default_med = float(train['price'].median()) if ('train' in globals() and 'price' in train.columns) else 10.0\n                pred_sample_blend = np.full(len(sample_test), default_med)\n                print(\"Fallback: using global median:\", default_med)\n        else:\n            # no train available? just use global value 10.0\n            pred_sample_blend = np.full(len(sample_test), 10.0)\n            print(\"No train available: fallback to constant 10.0 predictions.\")\n\n# 6) Ensure arrays are numpy and clipped positive\npred_test_blend = np.array(globals().get('pred_test_blend')) if 'pred_test_blend' in globals() else None\nif pred_test_blend is None:\n    # try find test blend or construct from other test preds if available\n    if 'pred_test_blend' in globals():\n        pred_test_blend = np.array(globals()['pred_test_blend'])\n    elif ('pred_test_lgb' in globals()) and ('pred_test_mlp' in globals()):\n        bw = globals().get('best_w', 0.5)\n        pred_test_blend = bw * np.array(globals()['pred_test_lgb']) + (1-bw) * np.array(globals()['pred_test_mlp'])\n        print(\"Constructed pred_test_blend from pred_test_lgb and pred_test_mlp using best_w.\")\n    else:\n        # try loading from files\n        pred_test_blend = try_load([os.path.join(WORK_DIR,'pred_test_blend.npy'), os.path.join(WORK_DIR,'pred_test_blend.csv')])\n        if pred_test_blend is None:\n            raise RuntimeError(\"pred_test_blend not found. You must produce pred_test_blend or pred_test_lgb/pred_test_mlp first.\")\n\npred_test_blend = np.clip(np.array(pred_test_blend).reshape(-1), 0.01, None)\npred_sample_blend = np.clip(np.array(pred_sample_blend).reshape(-1), 0.01, None)\n\n# 7) Save outputs\nout_test = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test_blend})\nout_sample = pd.DataFrame({'sample_id': sample_test['sample_id'], 'price': pred_sample_blend})\n\ntest_out_path = os.path.join(WORK_DIR, 'test_out.csv')\nsample_out_path = os.path.join(WORK_DIR, 'sample_test_out.csv')\n\nout_test.to_csv(test_out_path, index=False)\nout_sample.to_csv(sample_out_path, index=False)\n\nprint(\"Saved outputs to:\", test_out_path, sample_out_path)\nprint(\"test_out preview:\")\ndisplay(out_test.head())\nprint(\"sample_test_out preview:\")\ndisplay(out_sample.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:03:00.694191Z","iopub.execute_input":"2025-10-13T17:03:00.694893Z","iopub.status.idle":"2025-10-13T17:03:00.867199Z","shell.execute_reply.started":"2025-10-13T17:03:00.694869Z","shell.execute_reply":"2025-10-13T17:03:00.866370Z"}},"outputs":[{"name":"stdout","text":"pred_sample_blend not found in memory. Attempting to construct it...\npred_sample_lgb not found.\nFound pred_sample_mlp shape: (100,)\nUsing best_w from memory: 0.01\nOnly MLP sample preds found; using those as pred_sample_blend.\nSaved outputs to: /kaggle/working/test_out.csv /kaggle/working/sample_test_out.csv\ntest_out preview:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   sample_id      price\n0     100179  14.519507\n1     245611  14.131482\n2     146263  19.992171\n3      95658  10.229068\n4      36806  30.934236","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100179</td>\n      <td>14.519507</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>245611</td>\n      <td>14.131482</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>146263</td>\n      <td>19.992171</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95658</td>\n      <td>10.229068</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36806</td>\n      <td>30.934236</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"sample_test_out preview:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   sample_id      price\n0     217392  51.338554\n1     209156  16.353568\n2     262333   3.250908\n3     295979  17.740098\n4      50604  17.147069","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>217392</td>\n      <td>51.338554</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>209156</td>\n      <td>16.353568</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>262333</td>\n      <td>3.250908</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>295979</td>\n      <td>17.740098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50604</td>\n      <td>17.147069</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"out_test = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test_blend})\nout_sample = pd.DataFrame({'sample_id': sample_test['sample_id'], 'price': pred_sample_blend})\n\ntest_out_path = os.path.join(KAGGLE_WORKING_DIR, 'test_out.csv')\nsample_out_path = os.path.join(KAGGLE_WORKING_DIR, 'sample_test_out.csv')\n\nout_test.to_csv(test_out_path, index=False)\nout_sample.to_csv(sample_out_path, index=False)\nprint(\"Saved outputs to:\", test_out_path, sample_out_path)\ndisplay(out_test.head())\ndisplay(out_sample.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:57:32.344092Z","iopub.execute_input":"2025-10-13T16:57:32.344782Z","iopub.status.idle":"2025-10-13T16:57:32.362252Z","shell.execute_reply.started":"2025-10-13T16:57:32.344743Z","shell.execute_reply":"2025-10-13T16:57:32.361401Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/660948875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_test_blend\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_sample_blend\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_out_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKAGGLE_WORKING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_out.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_out_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKAGGLE_WORKING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_test_out.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pred_sample_blend' is not defined"],"ename":"NameError","evalue":"name 'pred_sample_blend' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"np.save(os.path.join(KAGGLE_WORKING_DIR, 'oof_pred_lgb.npy'), oof_pred_lgb)\nnp.save(os.path.join(KAGGLE_WORKING_DIR, 'oof_pred_mlp.npy'), oof_pred_mlp)\nnp.save(os.path.join(KAGGLE_WORKING_DIR, 'pred_test_blend.npy'), pred_test_blend)\nnp.save(os.path.join(KAGGLE_WORKING_DIR, 'pred_sample_blend.npy'), pred_sample_blend)\nprint(\"Saved OOF/pred arrays for debugging.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}