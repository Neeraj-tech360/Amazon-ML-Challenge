# ğŸ›’ Amazon ML Challenge 2025 â€“ Smart Product Pricing

## ğŸ“Œ Overview

E-commerce pricing depends on a complex mix of textual attributes (brand, pack size, specifications) and visual cues (packaging, perceived quality).
This project builds a **multimodal machine learning pipeline** to predict product prices using both **text and image data**.

**Objective:** Predict continuous product prices.
**Evaluation Metric:** SMAPE (Symmetric Mean Absolute Percentage Error)

Dataset Link: [https://www.kaggle.com/datasets/infiniper/amazon-ml-challenge-2025-dataset](https://www.kaggle.com/datasets/infiniper/amazon-ml-challenge-2025-dataset)

Repository Link: [https://github.com/Infiniper/Amazon-ML-Challenge-2025.git](https://github.com/Infiniper/Amazon-ML-Challenge-2025.git)

---

## ğŸ“Š Dataset

| Split | Size   | Description                      |
| ----- | ------ | -------------------------------- |
| Train | 75,000 | Product details + images + price |
| Test  | 75,000 | Product details (no price)       |

### Data Fields

* **sample_id** â€“ Unique product identifier
* **catalog_content** â€“ Title + description + item pack quantity
* **image_link** â€“ Public URL of product image
* **price** â€“ Continuous float value (target variable, train only)

---

## ğŸ§  Multimodal Architecture

### 1ï¸âƒ£ Raw Inputs

* Text: `catalog_content`
* Image: `image_link`
* Target: `price`

### 2ï¸âƒ£ Feature Engineering

Unstructured data is converted into numerical embeddings:

* **Regex Numeric Extraction** â†’ word count, char length, extracted numeric values & units
* **S-BERT (Text Embeddings)** â†’ 384-dimensional semantic vector
* **CLIP (Image Embeddings)** â†’ 512-dimensional visual feature vector

### 3ï¸âƒ£ Feature Stacking

All features are concatenated into a unified matrix (~900 features per product).

### 4ï¸âƒ£ Regression Model

A **LightGBM Regressor** learns non-linear relationships between stacked features and price.

---

## âš™ï¸ Training Strategy

### ğŸ”¹ Log Target Transformation

To handle skewed price distribution:

```
log_price = ln(1 + price)
```

Predictions are inverse-transformed after inference.

### ğŸ”¹ Stratified K-Fold (for Regression)

Log-prices are binned into quantiles to maintain balanced price distribution across folds.

### ğŸ”¹ Custom SMAPE Metric

SMAPE is implemented directly inside LightGBM for evaluation consistency.

### ğŸ”¹ Ensemble Bagging

Multiple models trained across different seeds and averaged to reduce variance.

---

## ğŸ’¡ Design Decisions

* **Why Multimodal?** Text misses visual cues; images miss quantity/scale. Fusion removes blind spots.
* **Why S-BERT & CLIP?** Transfer learning provides rich features without heavy training cost.
* **Why LightGBM?** Tree-based models outperform deep networks on structured embedding features.

---

## ğŸ“ˆ Evaluation Metric

```
SMAPE = (1/n) * Î£ |Actual - Predicted| / ((|Actual| + |Predicted|)/2) * 100%
```

* Range: 0% â€“ 200%
* Lower is better

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ dataset/                     # Sample CSVs and dataset info
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py                 # Image downloading utilities
â”‚   â””â”€â”€ example.ipynb            # Starter notebook
â”œâ”€â”€ Other_Notebooks/
â”‚   â”œâ”€â”€ light-gbm/               # LightGBM pipeline & feature extraction
â”‚   â””â”€â”€ SVM/                     # Alternative approach
â”œâ”€â”€ Results on test data/        # Prediction outputs
â”œâ”€â”€ main_notebook.ipynb          # Final multimodal pipeline
â”œâ”€â”€ download_dataset.py          # Dataset download script
â””â”€â”€ calculate_smape.py           # Local SMAPE evaluation
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/Infiniper/Amazon-ML-Challenge-2025.git
cd Amazon-ML-Challenge-2025
```

### 2ï¸âƒ£ Download Dataset

Download from Kaggle and place:

```
dataset/train.csv
dataset/test.csv
```

### 3ï¸âƒ£ Run Pipeline

Open:

```
main_notebook.ipynb
```

This notebook covers end-to-end:

* Data preprocessing
* Text & image embedding generation
* Feature stacking
* LightGBM training
* SMAPE evaluation

---

## ğŸ“Œ Notes

* No external price lookup was used.
* Model size constraints follow competition guidelines.
* Designed for reproducibility and modular experimentation.

---
